Index: database.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pymysql\n\n\ndef connection(user, password):\n    \"\"\"\"\"\"\n    con = pymysql.connect(host='localhost',\n                          user=user,\n                          password=password,\n                          cursorclass=pymysql.cursors.DictCursor)\n    return con\n\n\ndef create_database(con, name):\n    \"\"\"\"\"\"\n    with con.cursor() as cursor:\n        database = f'CREATE DATABASE {name}'\n        cursor.execute(database)\n\n\ndef create_table(con, database, name, *args):\n    \"\"\"\"\"\"\n    with con.cursor() as cursor:\n        select_database = f'USE {database}'\n        cursor.execute(select_database)\n        content = str(args).replace(\"'\", \"\")\n        table = f'CREATE TABLE {name} {content}'\n        cursor.execute(table)\n\n\ndef filling_table(con, database, table, variables, *data):\n    \"\"\"\"\"\"\n    with con.cursor() as cursor:\n        select_database = f\"USE {database}\"\n        cursor.execute(select_database)\n        variables = variables.replace(\"'\", \"\")\n        values = len(data)*'%s, '\n        values = values.rstrip(\", \")\n        fill_table = f\"REPLACE INTO {table} {variables} VALUES ({values})\"\n        cursor.execute(fill_table, [*data])\n        con.commit()\n\n\nif __name__ == '__main__':\n    connect = connection('root', 'rootroot')\n    create_database(connect, 'shufersal')\n    category_table_data = 'name VARCHAR(45) PRIMARY KEY', 'url VARCHAR(500)'\n    create_table(connect, 'shufersal', 'category', *category_table_data)\n    suppliers_table_data = 'id INT AUTO_INCREMENT PRIMARY KEY', 'name VARCHAR(45)'\n    create_table(connect, 'shufersal', 'suppliers', *suppliers_table_data)\n    product_details_data = 'id INT AUTO_INCREMENT PRIMARY KEY', 'name VARCHAR(45)', \\\n                           'FOREIGN KEY (id) REFERENCES suppliers(id)', 'FOREIGN KEY (name) REFERENCES category(name)'\n    create_table(connect, 'shufersal', 'product_details', *product_details_data)\n    product_price_data = 'id INT AUTO_INCREMENT PRIMARY KEY', 'price INT', 'price_unit INT',\\\n                         'price_unit_unit VARCHAR(45)', 'container VARCHAR(45)', 'date_time DATETIME',\\\n                         'FOREIGN KEY (id) REFERENCES product_details(id)'\n    create_table(connect, 'shufersal', 'product_price', *product_price_data)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/database.py b/database.py
--- a/database.py	(revision 2c8c698e03dbce41e4be54de13a3760e4a7823e9)
+++ b/database.py	(date 1669567166451)
@@ -41,7 +41,7 @@
 
 
 if __name__ == '__main__':
-    connect = connection('root', 'rootroot')
+    connect = connection('root', 'root1!2@')
     create_database(connect, 'shufersal')
     category_table_data = 'name VARCHAR(45) PRIMARY KEY', 'url VARCHAR(500)'
     create_table(connect, 'shufersal', 'category', *category_table_data)
Index: page_scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\npage_scraper: the script takes an url* of one of the subcategories in the Shufersal site and prints\nthe following attributes for each product in that category:\n    - product name\n    - price\n    - price unit\n    - container\n    - supplier\n\"\"\"\n\nimport time\nimport grequests\nfrom bs4 import BeautifulSoup\n# ____selenium____\nfrom selenium import webdriver\nfrom selenium.webdriver import ActionChains\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\nfrom database import connection\nfrom database import filling_table\n\n\n## main url\ncon = connection('root', 'rootroot')\nMAIN_URL = 'https://www.shufersal.co.il/online/he/S'\ngeneral_url = 'https://www.shufersal.co.il'\ncategories = 3\nLENGTH_GENERAL_URL = len(general_url)\n\ndef get_urls():\n    \"\"\"Finds the link to specific category websites.\"\"\"\n    options = Options()\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n    driver.maximize_window()\n    driver.get(MAIN_URL)\n    action = ActionChains(driver)\n    range_list = [(3, 15), (2, 10), (4, 15)]\n    category_urls = dict()\n    for i in range(categories):\n        ELEMENT = driver.find_element(By.XPATH, f'/ html / body / main / header / div[2] / nav / div / ul[1] / li[{i+2}]')\n        action.move_to_element(ELEMENT).perform()\n        time.sleep(1)\n        elements = range_list[i]\n        for j in range(*elements):\n            ELEMENT = driver.find_element(By.XPATH, f'// *[ @ id = \"secondMenu{i+2}\"] / li[{j}]')\n            action.move_to_element(ELEMENT).perform()\n            html = driver.page_source\n            soup = BeautifulSoup(html, \"lxml\")\n            menu_elements = soup.find_all('ul', class_=\"thirdMenu\")\n            for menu_element in menu_elements:\n                category_elements = menu_element.find_all('div', class_=\"content\")\n                for category_element in category_elements:\n                    category = category_element.find(\"a\").text.strip()\n                    url = category_element.find(\"a\")[\"href\"]\n                    category_url = url\n                    if url[1:7] == 'online':\n                        category_url = general_url + url\n                    if category_url[:LENGTH_GENERAL_URL] == general_url:\n                        category_urls[category] = category_url\n                        filling_table(con, 'shufersal', 'category', '(name, url)', category, category_url)\n    return category_urls\n\n\ndef parse_data(category_urls):\n    category_urls_values = list(category_urls.values())\n    for category_url in category_urls_values:\n        options = Options()\n        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n        driver.maximize_window()\n        driver.get(category_url)\n        for scroll in range(10):\n            SCROLL_PAUSE_TIME = 5\n            driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n            time.sleep(SCROLL_PAUSE_TIME)\n\n            html = driver.page_source\n            full_content = BeautifulSoup(html, \"lxml\")\n\n            class_type = ['miglog-prod miglog-sellingmethod-by_package', 'miglog-prod miglog-sellingmethod-by_weight',\n                  'miglog-prod miglog-sellingmethod-by_unit']\n            count = 0\n            for class_ in class_type:\n                products = full_content.find_all('li', class_=class_type)\n                print(f\"got {len(products)} products\")\n                for product in products:\n                    count += 1\n\n                    product_id = product['data-product-code']\n                    print(product_id)\n\n                    try:\n                        product_name = product.find('div', class_='text description').strong.text\n                        print(count, '.', product_name)\n                    except AttributeError as err:\n                        print(count, '.None')\n\n                    try:\n                        price = product.find('span', class_='price').span.text\n                        print(\"price:\", price.strip())\n                    except AttributeError as err:\n                        print(\"price: None\")\n\n                    try:\n                        priceUnit = product.find('span', class_='priceUnit').text\n                        print(\"priceUnit:\", priceUnit.strip())\n                    except AttributeError as err:\n                        print(\"priceUnit: None\")\n\n                    try:\n                        row = product.find('div', class_='labelsListContainer').div\n                        container = row.find_all('span')\n                        if type(container) == str or len(container) < 2:\n                            try:\n                                supplier = row.span.text\n                                print(\"supplier:\", supplier.strip())\n                            except AttributeError as err:\n                                print(\"supplier: None\")\n\n                        else:\n                            try:\n                                container = row.find_all('span')[0].text\n                                print(\"container:\", container.strip())\n                            except AttributeError as err:\n                                print(\"container: None\")\n\n                            try:\n                                supplier = row.find_all('span')[1].text\n                                print(\"supplier:\", supplier.strip())\n                            except AttributeError as err:\n                                print(\"supplier: None\")\n\n                    except AttributeError as err:\n                        print(\"priceUnit: None\")\n                        print(\"container: None\")\n\n                    print()\n\n\nif __name__ == '__main__':\n    U = get_urls()\n#    parse_data(U)\n\n#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)\n#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)\n#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)\n#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/page_scraper.py b/page_scraper.py
--- a/page_scraper.py	(revision 2c8c698e03dbce41e4be54de13a3760e4a7823e9)
+++ b/page_scraper.py	(date 1669567397844)
@@ -23,7 +23,7 @@
 
 
 ## main url
-con = connection('root', 'rootroot')
+con = connection('root', 'root1!2@')
 MAIN_URL = 'https://www.shufersal.co.il/online/he/S'
 general_url = 'https://www.shufersal.co.il'
 categories = 3
Index: Getting_links.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nThe script extracts all relevant urls addresses from the online supermarket of \"shufersal\"\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nMAIN_URL = 'https://www.shufersal.co.il/online/he/S'\n\n\ndef from_url_to_soup(url_address):\n    \"\"\"\n     from_url_to_soup gets an url address and returns it html content using BeautifulSoup4\n\n    :param url_address:  url address string\n    :return: soup - class 'bs4.BeautifulSoup'\n    \"\"\"\n    r = requests.get(url_address)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n    return soup\n\n\ndef main():\n    \"\"\"\n    main runs a search through Shufersal online main header and returns all the links to it sub categories\n    and the name of the categories (in hebrew). all the data is stored in shufersal_links.csv\n    \"\"\"\n\n    csv_file = open('shufersal_links.csv', 'w')\n    csv_writer = csv.writer(csv_file)\n    csv_writer.writerow(['id', 'Destination', 'Link'])\n\n    full_content = from_url_to_soup(MAIN_URL)\n    sub_panels = full_content.find_all('li', class_=\"second-level-li panel\")\n\n    count = 0\n    label_list = []\n    for sub_panel in sub_panels:\n        label = sub_panel.a.text.strip()\n        if label in label_list:\n            break\n        else:\n            label_list.append(label)\n            link = sub_panel.a.get('href')\n            if link[1:7] == 'online':\n                link = 'https://www.shufersal.co.il' + link\n            count += 1\n            csv_writer.writerow([count, label, link])\n            print(\"_______________________________________\")\n            print(label)\n            print(link)\n    print(label_list)\n    csv_file.close()\n\n    if count <= 20:\n        raise Exception(f\" only {count} links were scraped, please check the script/file/site...\")\n    else:\n        print(f\"{count} links were scraped.\")\n\n\nif __name__ == '__main__':\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Getting_links.py b/Getting_links.py
--- a/Getting_links.py	(revision 2c8c698e03dbce41e4be54de13a3760e4a7823e9)
+++ b/Getting_links.py	(date 1669567033993)
@@ -6,11 +6,12 @@
 from bs4 import BeautifulSoup
 import csv
 MAIN_URL = 'https://www.shufersal.co.il/online/he/S'
+MINIMUM_NUMBER_OF_LINKS = 20
 
 
 def from_url_to_soup(url_address):
     """
-     from_url_to_soup gets an url address and returns it html content using BeautifulSoup4
+     get_soup_from_url gets an url address and returns it html content using BeautifulSoup4
 
     :param url_address:  url address string
     :return: soup - class 'bs4.BeautifulSoup'
@@ -26,15 +27,17 @@
     and the name of the categories (in hebrew). all the data is stored in shufersal_links.csv
     """
 
-    csv_file = open('shufersal_links.csv', 'w')
-    csv_writer = csv.writer(csv_file)
-    csv_writer.writerow(['id', 'Destination', 'Link'])
+    # csv_file = open('shufersal_links.csv', 'w')
+    # csv_writer = csv.writer(csv_file)
+    # csv_writer.writerow(['id', 'Destination', 'Link'])
 
     full_content = from_url_to_soup(MAIN_URL)
     sub_panels = full_content.find_all('li', class_="second-level-li panel")
 
     count = 0
     label_list = []
+    links_list = []
+    category_list = []
     for sub_panel in sub_panels:
         label = sub_panel.a.text.strip()
         if label in label_list:
@@ -45,17 +48,23 @@
             if link[1:7] == 'online':
                 link = 'https://www.shufersal.co.il' + link
             count += 1
+            links_list.append(link)
+            category_list.append(label)
             csv_writer.writerow([count, label, link])
-            print("_______________________________________")
-            print(label)
-            print(link)
+
+
+
+
+
     print(label_list)
     csv_file.close()
 
-    if count <= 20:
+    if count <= MINIMUM_NUMBER_OF_LINKS:
         raise Exception(f" only {count} links were scraped, please check the script/file/site...")
     else:
-        print(f"{count} links were scraped.")
+        print(f"{len(links_list)} categories and their links were scraped.")
+
+    return links_list, category_list
 
 
 if __name__ == '__main__':
