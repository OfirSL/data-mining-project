Index: database.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pymysql\nimport sys\n\n\ndef connection(user, password):\n    \"\"\"'connection' gets a user and a password to MySQL Server\n    and returns a pymysql.connection.connection attribute\"\"\"\n\n    try:\n        con = pymysql.connect(host='localhost',\n                              user=user,\n                              password=password,\n                              cursorclass=pymysql.cursors.DictCursor)\n    except RuntimeError as err:\n        print('!!')\n    return con\n\n\ndef create_database(con, name):\n    \"\"\"'create_database' get a pymysql.connection.connection attribute and a name and creates an sql database, called by the name input\"\"\"\n\n    try:\n        with con.cursor() as cursor:\n            database = f'CREATE DATABASE {name}'\n            cursor.execute(database)\n    except pymysql.err.ProgrammingError as e:\n        print(f'ERROR {e.args[0]}: {e.args[1]}')\n        raise pymysql.err.ProgrammingError\n\n\ndef create_table(con, database, name, *args):\n    \"\"\"'attribute' get a pymysql.connection.connection attribute,\n    name of a database, name of a new table and\n    a list of arguments containing an sql code to implement on the new table\"\"\"\n\n    with con.cursor() as cursor:\n        select_database = f'USE {database}'\n        cursor.execute(select_database)\n        content = str(args).replace(\"'\", \"\")\n        table = f'CREATE TABLE {name} {content}'\n        cursor.execute(table)\n\n\ndef filling_table(con, database, table, variables, *data):\n    \"\"\"'filling_table' get a pymysql.connection.connection attribute,\n    name of a database, name of a table, string of variables and a list of data\n    and add it to the table \"\"\"\n\n    with con.cursor() as cursor:\n        select_database = f\"USE {database}\"\n        cursor.execute(select_database)\n        variables = variables.replace(\"'\", \"\")\n        values = len(data) * '%s, '\n        values = values.rstrip(\", \")\n        fill_table = f\"INSERT INTO {table} {variables} VALUES ({values})\"\n        cursor.execute(fill_table, [*data])\n        con.commit()\n\ndef delete_database(user, password, database):\n    con = connection(user, password)\n\n    with con.cursor() as cursor:\n        delete_database_sql = f\"DROP DATABASE {database};\"\n        cursor.execute(delete_database_sql)\n\n    return\n\ndef main(user, password):\n    \"\"\"\n    Main function of the module, creates the foundation of the 'shufersal' database using the MySQL Server\n    \"\"\"\n\n    connect = connection(user, password)\n    database_name = 'shufersal'\n\n    try:\n        create_database(connect, database_name)\n    except pymysql.err.ProgrammingError as e:\n        raise pymysql.err.ProgrammingError\n    category_table_data = 'id INT AUTO_INCREMENT PRIMARY KEY', 'name VARCHAR(45)', 'url VARCHAR(200)'\n    create_table(connect, database_name, 'category', *category_table_data)\n    suppliers_table_data = 'id INT AUTO_INCREMENT PRIMARY KEY', 'name VARCHAR(45)'\n    create_table(connect, database_name, 'suppliers', *suppliers_table_data)\n    product_details_data = 'id INT AUTO_INCREMENT PRIMARY KEY', 'name VARCHAR(45)', \\\n                           'FOREIGN KEY (id) REFERENCES suppliers(id)', 'FOREIGN KEY (id) REFERENCES category(id)'\n    create_table(connect, database_name, 'product_details', *product_details_data)\n    product_price_data = 'id INT AUTO_INCREMENT PRIMARY KEY', 'price INT', 'price_unit INT', \\\n                         'price_unit_unit VARCHAR(45)', 'container VARCHAR(45)', 'date_time DATETIME', \\\n                         'FOREIGN KEY (id) REFERENCES product_details(id)'\n    create_table(connect, database_name, 'product_price', *product_price_data)\n\n\nif __name__ == '__main__':\n    main(sys.argv[1], sys.argv[2])\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/database.py b/database.py
--- a/database.py	(revision 01a22d5f00f85c76332a9ca186d3a43c7bc0ca1e)
+++ b/database.py	(date 1669575730907)
@@ -70,6 +70,7 @@
     Main function of the module, creates the foundation of the 'shufersal' database using the MySQL Server
     """
 
+if __name__ == '__main__':
     connect = connection(user, password)
     database_name = 'shufersal'
 
Index: page_scraper.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\npage_scraper: the script takes an url* of one of the subcategories in the Shufersal site and prints\nthe following attributes for each product in that category:\n    - product name\n    - price\n    - price unit\n    - container\n    - supplier\n\"\"\"\n\nimport time\nimport requests\nfrom bs4 import BeautifulSoup\n# ____selenium____\nfrom selenium import webdriver\nfrom selenium.webdriver import ActionChains\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.chrome.service import Service\nfrom webdriver_manager.chrome import ChromeDriverManager\n\n## main url\nMAIN_URL = \"https://www.shufersal.co.il/online/he/%D7%A7%D7%98%D7%92%D7%95%D7%A8%D7%99%D7%95%D7%AA/%D7%A1%D7%95%D7%A4%D7%A8%D7%9E%D7%A8%D7%A7%D7%98/%D7%A4%D7%99%D7%A8%D7%95%D7%AA-%D7%95%D7%99%D7%A8%D7%A7%D7%95%D7%AA/c/A04\"\n\n\n\ndef main():\n    # ---- activate selenium____\n    DELAY = 50\n    options = Options()\n    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n    driver.set_window_size(1920, 1080)\n    driver.get(MAIN_URL)\n    actions = ActionChains(driver)\n\n    for scroll in range(10):\n        SCROLL_PAUSE_TIME = 5\n        driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n        time.sleep(SCROLL_PAUSE_TIME)\n\n    # # Get scroll height\n    # last_height = driver.execute_script(\"return document.body.scrollHeight\")\n    # SCROLL_PAUSE_TIME = 5\n    #\n    # while True:\n    #     # Scroll down to bottom\n    #     driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\")\n    #\n    #     # Wait to load page\n    #     time.sleep(SCROLL_PAUSE_TIME)\n    #\n    #     # Calculate new scroll height and compare with last scroll height\n    #     new_height = driver.execute_script(\"return document.body.scrollHeight\")\n    #     if new_height == last_height:\n    #         break\n    #     last_height = new_height\n\n    html = driver.page_source\n    full_content = BeautifulSoup(html, \"lxml\")\n\n    class_type = ['miglog-prod miglog-sellingmethod-by_package', 'miglog-prod miglog-sellingmethod-by_weight',\n                  'miglog-prod miglog-sellingmethod-by_unit']\n    count = 0\n    for class_ in class_type:\n        products = full_content.find_all('li', class_=class_type)\n        print(f\"got {len(products)} products\")\n        for product in products:\n            count += 1\n\n            try:\n                product_name = product.find('div', class_='text description').strong.text\n                print(count, '.', product_name)\n            except AttributeError as err:\n                print(count, '.None')\n\n            try:\n                price = product.find('span', class_='price').span.text\n                print(\"price:\", price.strip())\n            except AttributeError as err:\n                print(\"price: None\")\n\n            try:\n                priceUnit = product.find('span', class_='priceUnit').text\n                print(\"priceUnit:\", priceUnit.strip())\n            except AttributeError as err:\n                print(\"priceUnit: None\")\n\n            try:\n                row = product.find('div', class_='labelsListContainer').div\n                container = row.find_all('span')\n                if type(container) == str or len(container) < 2:\n                    try:\n                        supplier = row.span.text\n                        print(\"supplier:\", supplier.strip())\n                    except AttributeError as err:\n                        print(\"supplier: None\")\n\n                else:\n                    try:\n                        container = row.find_all('span')[0].text\n                        print(\"container:\", container.strip())\n                    except AttributeError as err:\n                        print(\"container: None\")\n\n                    try:\n                        supplier = row.find_all('span')[1].text\n                        print(\"supplier:\", supplier.strip())\n                    except AttributeError as err:\n                        print(\"supplier: None\")\n\n            except AttributeError as err:\n                print(\"priceUnit: None\")\n                print(\"container: None\")\n\n            print()\n\n\nif __name__ == '__main__':\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/page_scraper.py b/page_scraper.py
--- a/page_scraper.py	(revision 01a22d5f00f85c76332a9ca186d3a43c7bc0ca1e)
+++ b/page_scraper.py	(date 1669575731004)
@@ -9,110 +9,142 @@
 """
 
 import time
-import requests
+import grequests
 from bs4 import BeautifulSoup
 # ____selenium____
 from selenium import webdriver
 from selenium.webdriver import ActionChains
+from selenium.webdriver.common.by import By
 from selenium.webdriver.chrome.options import Options
 from selenium.webdriver.chrome.service import Service
 from webdriver_manager.chrome import ChromeDriverManager
+from database import connection
+from database import filling_table
+
 
 ## main url
-MAIN_URL = "https://www.shufersal.co.il/online/he/%D7%A7%D7%98%D7%92%D7%95%D7%A8%D7%99%D7%95%D7%AA/%D7%A1%D7%95%D7%A4%D7%A8%D7%9E%D7%A8%D7%A7%D7%98/%D7%A4%D7%99%D7%A8%D7%95%D7%AA-%D7%95%D7%99%D7%A8%D7%A7%D7%95%D7%AA/c/A04"
+con = connection('root', 'rootroot')
+MAIN_URL = 'https://www.shufersal.co.il/online/he/S'
+general_url = 'https://www.shufersal.co.il'
+LENGTH_GENERAL_URL = len(general_url)
+categories = 3
+range_list = [(3, 15), (2, 10), (4, 15)]
 
 
-
-def main():
-    # ---- activate selenium____
-    DELAY = 50
+def get_urls():
+    """Finds the link to specific category websites."""
     options = Options()
     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
-    driver.set_window_size(1920, 1080)
+    driver.maximize_window()
     driver.get(MAIN_URL)
-    actions = ActionChains(driver)
+    action = ActionChains(driver)
+    category_urls = dict()
+    for i in range(categories):
+        ELEMENT = driver.find_element(By.XPATH, f'/ html / body / main / header / div[2] / nav / div / ul[1] / li[{i+2}]')
+        action.move_to_element(ELEMENT).perform()
+        time.sleep(1)
+        elements = range_list[i]
+        for j in range(*elements):
+            ELEMENT = driver.find_element(By.XPATH, f'// *[ @ id = "secondMenu{i+2}"] / li[{j}]')
+            action.move_to_element(ELEMENT).perform()
+            html = driver.page_source
+            soup = BeautifulSoup(html, "lxml")
+            menu_elements = soup.find_all('ul', class_="thirdMenu")
+            for menu_element in menu_elements:
+                category_elements = menu_element.find_all('div', class_="content")
+                for category_element in category_elements:
+                    category = category_element.find("a").text.strip()
+                    url = category_element.find("a")["href"]
+                    category_url = url
+                    if url[1:7] == 'online':
+                        category_url = general_url + url
+                    if category_url[:LENGTH_GENERAL_URL] == general_url:
+                        category_urls[category] = category_url
+                        filling_table(con, 'shufersal', 'category', '(name, url)', category, category_url)
+    return category_urls
 
-    for scroll in range(10):
-        SCROLL_PAUSE_TIME = 5
-        driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
-        time.sleep(SCROLL_PAUSE_TIME)
+
+def parse_data(category_urls):
+    category_urls_keys = list(category_urls.keys())
+    for category_url in category_urls_keys:
+        options = Options()
+        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
+        driver.maximize_window()
+        driver.get(category_urls[category_url])
+        for scroll in range(10):
+            SCROLL_PAUSE_TIME = 5
+            driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
+            time.sleep(SCROLL_PAUSE_TIME)
 
-    # # Get scroll height
-    # last_height = driver.execute_script("return document.body.scrollHeight")
-    # SCROLL_PAUSE_TIME = 5
-    #
-    # while True:
-    #     # Scroll down to bottom
-    #     driver.execute_script("window.scrollTo(0,document.body.scrollHeight);")
-    #
-    #     # Wait to load page
-    #     time.sleep(SCROLL_PAUSE_TIME)
-    #
-    #     # Calculate new scroll height and compare with last scroll height
-    #     new_height = driver.execute_script("return document.body.scrollHeight")
-    #     if new_height == last_height:
-    #         break
-    #     last_height = new_height
-
-    html = driver.page_source
-    full_content = BeautifulSoup(html, "lxml")
+            html = driver.page_source
+            full_content = BeautifulSoup(html, "lxml")
 
-    class_type = ['miglog-prod miglog-sellingmethod-by_package', 'miglog-prod miglog-sellingmethod-by_weight',
+            class_type = ['miglog-prod miglog-sellingmethod-by_package', 'miglog-prod miglog-sellingmethod-by_weight',
                   'miglog-prod miglog-sellingmethod-by_unit']
-    count = 0
-    for class_ in class_type:
-        products = full_content.find_all('li', class_=class_type)
-        print(f"got {len(products)} products")
-        for product in products:
-            count += 1
+            count = 0
+            for class_ in class_type:
+                products = full_content.find_all('li', class_=class_type)
+                for product in products:
+                    count += 1
 
-            try:
-                product_name = product.find('div', class_='text description').strong.text
-                print(count, '.', product_name)
-            except AttributeError as err:
-                print(count, '.None')
+                    try:
+                        product_id = product['data-product-code']
+                    except AttributeError as err:
+                        product_id = 'NaN'
+
+                    try:
+                        product_name = product.find('div', class_='text description').strong.text
+                    except AttributeError as err:
+                        product_name = 'NaN'
 
-            try:
-                price = product.find('span', class_='price').span.text
-                print("price:", price.strip())
-            except AttributeError as err:
-                print("price: None")
+                    try:
+                        price = product.find('span', class_='price').span.text
+                    except AttributeError as err:
+                        price = 'NaN'
 
-            try:
-                priceUnit = product.find('span', class_='priceUnit').text
-                print("priceUnit:", priceUnit.strip())
-            except AttributeError as err:
-                print("priceUnit: None")
+                    try:
+                        priceUnit = product.find('span', class_='priceUnit').text
+                    except AttributeError as err:
+                        priceUnit = 'NaN'
 
-            try:
-                row = product.find('div', class_='labelsListContainer').div
-                container = row.find_all('span')
-                if type(container) == str or len(container) < 2:
-                    try:
-                        supplier = row.span.text
-                        print("supplier:", supplier.strip())
-                    except AttributeError as err:
-                        print("supplier: None")
+                    try:
+                        row = product.find('div', class_='labelsListContainer').div
+                        container = row.find_all('span')
+                        if type(container) == str or len(container) < 2:
+                            try:
+                                supplier = row.span.text
+                            except AttributeError as err:
+                                supplier = 'NaN'
 
-                else:
-                    try:
-                        container = row.find_all('span')[0].text
-                        print("container:", container.strip())
-                    except AttributeError as err:
-                        print("container: None")
+                        else:
+                            try:
+                                container = row.find_all('span')[0].text
+                            except AttributeError as err:
+                                container = 'NaN'
 
-                    try:
-                        supplier = row.find_all('span')[1].text
-                        print("supplier:", supplier.strip())
-                    except AttributeError as err:
-                        print("supplier: None")
+                            try:
+                                supplier = row.find_all('span')[1].text
+                            except AttributeError as err:
+                                supplier = 'NaN'
+
+
+                        filling_table(con, 'shufersal', 'suppliers', '(name)', supplier)
+                        filling_table(con, 'shufersal', 'products_details', '(id, name, id_supplier, id_categories)',
+                                      product_id, product_name, supplier, category_url)
+                        filling_table(con, 'shufersal', 'product_price', '(price, price_unit, container, product_id)',
+                                      price, priceUnit, container, product_id)
 
-            except AttributeError as err:
-                print("priceUnit: None")
-                print("container: None")
+                    except AttributeError as err:
+                        priceUnit = 'NaN'
+                        container = 'NaN'
 
-            print()
 
 
 if __name__ == '__main__':
-    main()
+    U = get_urls()
+#    parse_data(U)
+
+#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)
+#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)
+#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)
+#filling_table(con, 'shufersal', 'category', '(name, url)', product_name, price)
Index: Getting_links.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\nThe script extracts all relevant urls addresses from the online supermarket of \"shufersal\"\n\"\"\"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nMAIN_URL = 'https://www.shufersal.co.il/online/he/S'\n\n\ndef from_url_to_soup(url_address):\n    \"\"\"\n     from_url_to_soup gets an url address and returns it html content using BeautifulSoup4\n\n    :param url_address:  url address string\n    :return: soup - class 'bs4.BeautifulSoup'\n    \"\"\"\n    r = requests.get(url_address)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n    return soup\n\ndef main():\n    \"\"\"\n    main runs a search through Shufersal online main header and returns all the links to it sub categories\n    and the name of the categories (in hebrew). all the data is stored in shufersal_links.csv\n    \"\"\"\n\n    csv_file = open('shufersal_links.csv', 'w')\n    csv_writer = csv.writer(csv_file)\n    csv_writer.writerow(['id', 'Destination', 'Link'])\n\n    full_content = from_url_to_soup(MAIN_URL)\n    sub_panels = full_content.find_all('li', class_=\"second-level-li panel\")\n\n    count = 0\n    label_list = []\n    for sub_panel in sub_panels:\n        label = sub_panel.a.text.strip()\n        if label in label_list:\n            break\n        else:\n            label_list.append(label)\n            link = sub_panel.a.get('href')\n            if link[1:7] == 'online':\n                link = 'https://www.shufersal.co.il' + link\n            count += 1\n            csv_writer.writerow([count, label, link])\n            print(\"_______________________________________\")\n            print(label)\n            print(link)\n\n    csv_file.close()\n\n    if count <= 20:\n        raise Exception(f\" only {count} links were scraped, please check the script/file/site...\")\n    else:\n        print(f\"{count} links were scraped.\")\n\n\nif __name__ == '__main__':\n    main()\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Getting_links.py b/Getting_links.py
--- a/Getting_links.py	(revision 01a22d5f00f85c76332a9ca186d3a43c7bc0ca1e)
+++ b/Getting_links.py	(date 1669575731008)
@@ -6,11 +6,12 @@
 from bs4 import BeautifulSoup
 import csv
 MAIN_URL = 'https://www.shufersal.co.il/online/he/S'
+MINIMUM_NUMBER_OF_LINKS = 20
 
 
 def from_url_to_soup(url_address):
     """
-     from_url_to_soup gets an url address and returns it html content using BeautifulSoup4
+     get_soup_from_url gets an url address and returns it html content using BeautifulSoup4
 
     :param url_address:  url address string
     :return: soup - class 'bs4.BeautifulSoup'
@@ -25,15 +26,17 @@
     and the name of the categories (in hebrew). all the data is stored in shufersal_links.csv
     """
 
-    csv_file = open('shufersal_links.csv', 'w')
-    csv_writer = csv.writer(csv_file)
-    csv_writer.writerow(['id', 'Destination', 'Link'])
+    # csv_file = open('shufersal_links.csv', 'w')
+    # csv_writer = csv.writer(csv_file)
+    # csv_writer.writerow(['id', 'Destination', 'Link'])
 
     full_content = from_url_to_soup(MAIN_URL)
     sub_panels = full_content.find_all('li', class_="second-level-li panel")
 
     count = 0
     label_list = []
+    links_list = []
+    category_list = []
     for sub_panel in sub_panels:
         label = sub_panel.a.text.strip()
         if label in label_list:
@@ -44,17 +47,23 @@
             if link[1:7] == 'online':
                 link = 'https://www.shufersal.co.il' + link
             count += 1
+            links_list.append(link)
+            category_list.append(label)
             csv_writer.writerow([count, label, link])
-            print("_______________________________________")
-            print(label)
-            print(link)
+
+
+
+
+
 
     csv_file.close()
 
-    if count <= 20:
+    if count <= MINIMUM_NUMBER_OF_LINKS:
         raise Exception(f" only {count} links were scraped, please check the script/file/site...")
     else:
-        print(f"{count} links were scraped.")
+        print(f"{len(links_list)} categories and their links were scraped.")
+
+    return links_list, category_list
 
 
 if __name__ == '__main__':
